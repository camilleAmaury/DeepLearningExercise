{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "\n",
    "This notebook was created by Camille-Amaury JUGE, in order to better understand CNN principles and how they work.\n",
    "\n",
    "(it follows the exercices proposed by Hadelin de Ponteves on Udemy : https://www.udemy.com/course/le-deep-learning-de-a-a-z/)\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# scikit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "# keras\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Flatten\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "### Basic Convolutional Network\n",
    "\n",
    "Since our images are in RGB mode, we need to have an array of dimension 3 (one dimension for each color mode).\n",
    "\n",
    "We will also begin as a small but efficient size of feature map = 32 (should multiplicate by 2 if stacking multiple layer).\n",
    "\n",
    "Then, the feature dectetor will be 3*3 pixels.\n",
    "\n",
    "Finally, we always use the relu function in order to break linearity of features map. This improve the quality of the features.\n",
    "\n",
    "Thus, we apply max pooling in order to keep the important information while reducing the size of inputs (even if we lose some informations). We will use a 2*2 matrix which will reduce by 4 the features size.\n",
    "\n",
    "To conclude, the flattening layer will help the features to be flatten in order to be used by other kind of neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_convolutional_layer(clf):\n",
    "    # convolution layer\n",
    "    clf.add(Convolution2D(filters=32, kernel_size=(3,3), strides=(1,1),\n",
    "                         input_shape=(64,64,3), activation=\"relu\"))\n",
    "    # Pooling (here max)\n",
    "    clf.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    # Flattening \n",
    "    clf.add(Flatten())\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_layer(clf):\n",
    "    clf.add(Dense(units=128, activation=\"relu\"))\n",
    "    clf.add(Dense(units=1, activation=\"sigmoid\"))\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Sequential()\n",
    "clf = create_convolutional_layer(clf)\n",
    "clf = hidden_layer(clf)\n",
    "clf.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Creation\n",
    "\n",
    "We are going to increase the number of images by using a keras method which apply a lot of filters, rotation on existing images in order to avoid overfitting and prepare for lot of different factors.\n",
    "\n",
    "Folders need to be well organized before using those functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Epoch 1/25\n",
      "250/250 [==============================] - 40s 161ms/step - loss: 0.6819 - accuracy: 0.6281 - val_loss: 0.6625 - val_accuracy: 0.6875\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 0.5893 - accuracy: 0.6871 - val_loss: 0.5655 - val_accuracy: 0.7812\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 39s 156ms/step - loss: 0.5572 - accuracy: 0.7097 - val_loss: 0.5740 - val_accuracy: 0.7552\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 40s 160ms/step - loss: 0.5383 - accuracy: 0.7236 - val_loss: 0.6166 - val_accuracy: 0.6667\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 39s 158ms/step - loss: 0.5261 - accuracy: 0.7330 - val_loss: 0.5532 - val_accuracy: 0.7240\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 40s 158ms/step - loss: 0.5067 - accuracy: 0.7467 - val_loss: 0.6338 - val_accuracy: 0.7396\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 0.4935 - accuracy: 0.7602 - val_loss: 0.8266 - val_accuracy: 0.6979\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 0.4880 - accuracy: 0.7607 - val_loss: 0.6511 - val_accuracy: 0.7292\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 0.4789 - accuracy: 0.7681 - val_loss: 0.3234 - val_accuracy: 0.8125\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 0.4652 - accuracy: 0.7766 - val_loss: 0.6970 - val_accuracy: 0.7083\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 0.4516 - accuracy: 0.7814 - val_loss: 0.6109 - val_accuracy: 0.7045\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 0.4400 - accuracy: 0.7894 - val_loss: 0.6008 - val_accuracy: 0.7240\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 40s 160ms/step - loss: 0.4368 - accuracy: 0.7919 - val_loss: 0.8036 - val_accuracy: 0.7500\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 40s 158ms/step - loss: 0.4217 - accuracy: 0.7981 - val_loss: 0.4839 - val_accuracy: 0.7344\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 0.4135 - accuracy: 0.8087 - val_loss: 0.4381 - val_accuracy: 0.7708\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 40s 161ms/step - loss: 0.4079 - accuracy: 0.8136 - val_loss: 0.3330 - val_accuracy: 0.8021\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 41s 163ms/step - loss: 0.3901 - accuracy: 0.8248 - val_loss: 0.5646 - val_accuracy: 0.7604\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 41s 163ms/step - loss: 0.3878 - accuracy: 0.8285 - val_loss: 0.5941 - val_accuracy: 0.7448\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 41s 163ms/step - loss: 0.3692 - accuracy: 0.8320 - val_loss: 0.6090 - val_accuracy: 0.7708\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 41s 164ms/step - loss: 0.3511 - accuracy: 0.8479 - val_loss: 0.2442 - val_accuracy: 0.7865\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 40s 161ms/step - loss: 0.3490 - accuracy: 0.8426 - val_loss: 0.4292 - val_accuracy: 0.7614\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 41s 165ms/step - loss: 0.3388 - accuracy: 0.8512 - val_loss: 0.4262 - val_accuracy: 0.8490\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 41s 164ms/step - loss: 0.3188 - accuracy: 0.8609 - val_loss: 0.3888 - val_accuracy: 0.7396\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 41s 163ms/step - loss: 0.3081 - accuracy: 0.8662 - val_loss: 0.7291 - val_accuracy: 0.7448\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 41s 163ms/step - loss: 0.3064 - accuracy: 0.8671 - val_loss: 0.3930 - val_accuracy: 0.7812\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1ef99a2f088>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_batch_size = 32\n",
    "_training_size = 8000\n",
    "_test_size = 200\n",
    "_image_size = (64,64)\n",
    "\n",
    "# change/create the train dataset images\n",
    "train_datagen = ImageDataGenerator(\n",
    "    # rescale the values of each pixel between 0 and 1\n",
    "    rescale=1./255,\n",
    "    # transvection (rotating in 3D but still seing in 2D)\n",
    "    shear_range=0.2,\n",
    "    # zoom\n",
    "    zoom_range=0.2,\n",
    "    # return the image on a horizontal plan\n",
    "    horizontal_flip=True)\n",
    "\n",
    "# same for the test set\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# generate the new images for train dataset\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'training_set',\n",
    "        target_size=_image_size,\n",
    "        batch_size=_batch_size,\n",
    "        class_mode='binary')\n",
    "# same for the test dataset\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        'test_set',\n",
    "        target_size=_image_size,\n",
    "        batch_size=_batch_size,\n",
    "        class_mode='binary')\n",
    "\n",
    "# do the job\n",
    "clf.fit(train_generator,\n",
    "        steps_per_epoch=int(_training_size/_batch_size),\n",
    "        epochs=25,\n",
    "        validation_data=test_generator,\n",
    "        validation_steps=int(_test_size/_batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the network performs medium-well on the image recognition : 0.86 on training set and 0.78 on test set.\n",
    "\n",
    "Nethertheless, it seems that there is overfitting since we have a 0.08 difference which is quite enormous. Our neural network is not able to well generalize.\n",
    "\n",
    "### Improving the model\n",
    "\n",
    "We will add some convolution layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_convolutional_layer(clf):\n",
    "    # convolution layer\n",
    "    clf.add(Convolution2D(filters=64, kernel_size=(3,3), strides=(1,1),\n",
    "                         input_shape=(128,128,3), activation=\"relu\"))\n",
    "    # Pooling (here max)\n",
    "    clf.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    clf.add(Dropout(0.1))\n",
    "    clf.add(Convolution2D(filters=32, kernel_size=(3,3), strides=(1,1),\n",
    "                          activation=\"relu\"))\n",
    "    clf.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    clf.add(Dropout(0.1))\n",
    "    # Flattening \n",
    "    clf.add(Flatten())\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_layer(clf):\n",
    "    clf.add(Dense(units=128, activation=\"relu\"))\n",
    "    clf.add(Dense(units=64, activation=\"relu\"))\n",
    "    clf.add(Dense(units=1, activation=\"sigmoid\"))\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Sequential()\n",
    "clf = create_convolutional_layer(clf)\n",
    "clf = hidden_layer(clf)\n",
    "clf.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Epoch 1/25\n",
      "250/250 [==============================] - 115s 459ms/step - loss: 0.6960 - accuracy: 0.5299 - val_loss: 0.6686 - val_accuracy: 0.5990\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 112s 449ms/step - loss: 0.6622 - accuracy: 0.6058 - val_loss: 0.5992 - val_accuracy: 0.7031\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 114s 457ms/step - loss: 0.6298 - accuracy: 0.6489 - val_loss: 0.6285 - val_accuracy: 0.6198\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 114s 454ms/step - loss: 0.6064 - accuracy: 0.6731 - val_loss: 0.6038 - val_accuracy: 0.6510\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 113s 451ms/step - loss: 0.5799 - accuracy: 0.7011 - val_loss: 0.4057 - val_accuracy: 0.7760\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 112s 448ms/step - loss: 0.5649 - accuracy: 0.7076 - val_loss: 0.6833 - val_accuracy: 0.7031\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 112s 448ms/step - loss: 0.5321 - accuracy: 0.7384 - val_loss: 0.4534 - val_accuracy: 0.7188\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 113s 452ms/step - loss: 0.5005 - accuracy: 0.7548 - val_loss: 0.6207 - val_accuracy: 0.7500\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 112s 448ms/step - loss: 0.4799 - accuracy: 0.7648 - val_loss: 0.8075 - val_accuracy: 0.7188\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 112s 449ms/step - loss: 0.4644 - accuracy: 0.7793 - val_loss: 0.4120 - val_accuracy: 0.7969\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 112s 449ms/step - loss: 0.4338 - accuracy: 0.7926 - val_loss: 0.3669 - val_accuracy: 0.7670\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 112s 448ms/step - loss: 0.4162 - accuracy: 0.8070 - val_loss: 0.5565 - val_accuracy: 0.7604\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 112s 449ms/step - loss: 0.4029 - accuracy: 0.8138 - val_loss: 0.4363 - val_accuracy: 0.7344\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 113s 452ms/step - loss: 0.3785 - accuracy: 0.8325 - val_loss: 0.4355 - val_accuracy: 0.7344\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 112s 447ms/step - loss: 0.3650 - accuracy: 0.8334 - val_loss: 0.4168 - val_accuracy: 0.7396\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 112s 448ms/step - loss: 0.3407 - accuracy: 0.8490 - val_loss: 0.5171 - val_accuracy: 0.7656\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 112s 448ms/step - loss: 0.3229 - accuracy: 0.8560 - val_loss: 0.3271 - val_accuracy: 0.8021\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 115s 462ms/step - loss: 0.3161 - accuracy: 0.8694 - val_loss: 0.6226 - val_accuracy: 0.7969\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 114s 457ms/step - loss: 0.2921 - accuracy: 0.8754 - val_loss: 0.2555 - val_accuracy: 0.8073\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 113s 450ms/step - loss: 0.2793 - accuracy: 0.8855 - val_loss: 0.4655 - val_accuracy: 0.7500\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 113s 451ms/step - loss: 0.2540 - accuracy: 0.8878 - val_loss: 0.6282 - val_accuracy: 0.7443\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 113s 451ms/step - loss: 0.2524 - accuracy: 0.8979 - val_loss: 0.4574 - val_accuracy: 0.8177\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 113s 451ms/step - loss: 0.2335 - accuracy: 0.9044 - val_loss: 0.7167 - val_accuracy: 0.8229\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 114s 455ms/step - loss: 0.2225 - accuracy: 0.9087 - val_loss: 0.4846 - val_accuracy: 0.8281\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 113s 451ms/step - loss: 0.2113 - accuracy: 0.9150 - val_loss: 0.5986 - val_accuracy: 0.7656\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1ef9d42bac8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_batch_size = 32\n",
    "_training_size = 8000\n",
    "_test_size = 200\n",
    "_image_size = (128,128)\n",
    "\n",
    "# change/create the train dataset images\n",
    "train_datagen = ImageDataGenerator(\n",
    "    # rescale the values of each pixel between 0 and 1\n",
    "    rescale=1./255,\n",
    "    # transvection (rotating in 3D but still seing in 2D)\n",
    "    shear_range=0.2,\n",
    "    # zoom\n",
    "    zoom_range=0.2,\n",
    "    # return the image on a horizontal plan\n",
    "    horizontal_flip=True)\n",
    "\n",
    "# same for the test set\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# generate the new images for train dataset\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'training_set',\n",
    "        target_size=_image_size,\n",
    "        batch_size=_batch_size,\n",
    "        class_mode='binary')\n",
    "# same for the test dataset\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        'test_set',\n",
    "        target_size=_image_size,\n",
    "        batch_size=_batch_size,\n",
    "        class_mode='binary')\n",
    "\n",
    "# do the job\n",
    "clf.fit(train_generator,\n",
    "        steps_per_epoch=int(_training_size/_batch_size),\n",
    "        epochs=25,\n",
    "        validation_data=test_generator,\n",
    "        validation_steps=int(_test_size/_batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict One image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image1_128 = image.load_img(\"single_prediction\\\\cat_or_dog_1.jpg\",\n",
    "                           target_size=(128,128))\n",
    "test_image2_128 = image.load_img(\"single_prediction\\\\cat_or_dog_2.jpg\",\n",
    "                           target_size=(128,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image1_128 = image.img_to_array(test_image1_128)\n",
    "test_image2_128 = image.img_to_array(test_image2_128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image1_128 = np.expand_dims(test_image1_128, axis=0)\n",
    "test_image2_128 = np.expand_dims(test_image2_128, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 128, 128, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image1_128.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'cats', 1: 'dogs'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = {value : key for (key, value) in train_generator.class_indices.items()}\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted dogs and is dogs\n",
      "Predicted cats and is cats\n"
     ]
    }
   ],
   "source": [
    "y_pred = classes[int(clf.predict(test_image1_128)[0][0])]\n",
    "y = classes[1]\n",
    "print(\"Predicted {} and is {}\".format(y_pred, y))\n",
    "y_pred = classes[int(clf.predict(test_image2_128)[0][0])]\n",
    "y = classes[0]\n",
    "print(\"Predicted {} and is {}\".format(y_pred, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
